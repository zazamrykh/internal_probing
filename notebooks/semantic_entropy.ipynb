{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7721d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncertainty: /home/zazamrykh/projects/internal_probing/external/semantic_uncertainty/semantic_uncertainty/uncertainty/__init__.py\n",
      "semantic_entropy imports OK\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent  # если CWD = notebooks\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"external\" / \"semantic_uncertainty\"))\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"external\" / \"semantic_uncertainty\" / \"semantic_uncertainty\"))\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"external\" / \"semantic_uncertainty\" / \"semantic_uncertainty\" / \"uncertainty\"))\n",
    "\n",
    "\n",
    "import semantic_uncertainty\n",
    "import uncertainty\n",
    "\n",
    "print(\"uncertainty:\", getattr(uncertainty, \"__file__\", None))\n",
    "\n",
    "from semantic_uncertainty.uncertainty.uncertainty_measures.semantic_entropy import (\n",
    "    EntailmentDeberta,\n",
    "    get_semantic_ids,\n",
    "    logsumexp_by_id,\n",
    "    predictive_entropy_rao,\n",
    ")\n",
    "print(\"semantic_entropy imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07a3e6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1+cu128\n",
      "cuda available: True\n",
      "gpu: NVIDIA GeForce RTX 3060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "def pick_dtype():\n",
    "    # bf16 работает только на GPU с поддержкой bf16 (обычно Ampere/Hopper и новее).\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "        return torch.bfloat16\n",
    "    return torch.float16\n",
    "\n",
    "DTYPE = pick_dtype()\n",
    "DTYPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a66e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "238f5086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72082e045eff4a7eaa6e2b62b5934bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/mandarjoshi/trivia_qa/resolve/0f7faf33a3908546c6fd5b73a660e0f8ff173c2f/rc.nocontext/validation-00000-of-00001.parquet\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " dict_keys(['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer']))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from itertools import islice\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "ds_stream = load_dataset(\n",
    "    \"mandarjoshi/trivia_qa\",\n",
    "    \"rc.nocontext\",\n",
    "    split=\"validation\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "ds_stream = ds_stream.shuffle(seed=SEED, buffer_size=2_000)\n",
    "\n",
    "sample = list(islice(ds_stream, 10))\n",
    "\n",
    "len(sample), sample[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8785dfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>gold_normalized</th>\n",
       "      <th>gold_value_raw</th>\n",
       "      <th>gold_aliases_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tc_2167</td>\n",
       "      <td>Which US city was named after a British Prime ...</td>\n",
       "      <td>[steel city, climate of pittsburgh pennsylvani...</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>[Smoky City, Pittsburgh (Pa.), Pittsburgh, Pen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qb_9081</td>\n",
       "      <td>Harold Holt became Prime Minister of which cou...</td>\n",
       "      <td>[australie, orstraya, federal australia, austr...</td>\n",
       "      <td>Australia</td>\n",
       "      <td>[Australia (Commonwealth realm), AustraliA, Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qb_6552</td>\n",
       "      <td>The Suez Canal joins the Red Sea and which oth...</td>\n",
       "      <td>[sea of mediterranea, mediterranian sea, roman...</td>\n",
       "      <td>Mediterranean Sea</td>\n",
       "      <td>[Mediterranian, Meditiranean, West Mediterrane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qb_2020</td>\n",
       "      <td>Selenology is the scientific study of which ce...</td>\n",
       "      <td>[moonless, earth and moon, lunar mass, luna sa...</td>\n",
       "      <td>The moon</td>\n",
       "      <td>[Sol 3a, Moon-like, Mass of the Moon, Solar an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qb_1830</td>\n",
       "      <td>In September 2006 the government of Prime Mini...</td>\n",
       "      <td>[thailand, kingdom of thailand, kingdom of tha...</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>[Muang Thai, Taihland, ISO 3166-1:TH, Thai Emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dpql_452</td>\n",
       "      <td>Anatomy. Where are the intercostal muscles sit...</td>\n",
       "      <td>[between ribs]</td>\n",
       "      <td>Between the RIBS</td>\n",
       "      <td>[Between the RIBS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tc_2090</td>\n",
       "      <td>Who first drew Mickey Mouse when ?Disney first...</td>\n",
       "      <td>[celebrity productions, iwerks ub, ub iwerks, ...</td>\n",
       "      <td>Ub Iwerks</td>\n",
       "      <td>[Iwerks, Ub, Ub Iwerks, Ub Iwerks Studio, Cele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>qb_7419</td>\n",
       "      <td>A quadruped is an animal with how many feet?</td>\n",
       "      <td>[four, 4]</td>\n",
       "      <td>Four</td>\n",
       "      <td>[Four, four, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>qz_2194</td>\n",
       "      <td>Who was part man, part machine, all cop and ha...</td>\n",
       "      <td>[robocop 1987 film, robotic police officer, ro...</td>\n",
       "      <td>Robocop</td>\n",
       "      <td>[I'd buy that for a dollar, RoboCop, RobotCop,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tc_2250</td>\n",
       "      <td>What kind of disaster claimed some 100,000 liv...</td>\n",
       "      <td>[地震, earth quakes, earthquakes, tectonic earth...</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>[Seism, Earthquake, Seismic event, The kinds o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_id                                           question  \\\n",
       "0     tc_2167  Which US city was named after a British Prime ...   \n",
       "1     qb_9081  Harold Holt became Prime Minister of which cou...   \n",
       "2     qb_6552  The Suez Canal joins the Red Sea and which oth...   \n",
       "3     qb_2020  Selenology is the scientific study of which ce...   \n",
       "4     qb_1830  In September 2006 the government of Prime Mini...   \n",
       "5    dpql_452  Anatomy. Where are the intercostal muscles sit...   \n",
       "6     tc_2090  Who first drew Mickey Mouse when ?Disney first...   \n",
       "7     qb_7419       A quadruped is an animal with how many feet?   \n",
       "8     qz_2194  Who was part man, part machine, all cop and ha...   \n",
       "9     tc_2250  What kind of disaster claimed some 100,000 liv...   \n",
       "\n",
       "                                     gold_normalized     gold_value_raw  \\\n",
       "0  [steel city, climate of pittsburgh pennsylvani...         Pittsburgh   \n",
       "1  [australie, orstraya, federal australia, austr...          Australia   \n",
       "2  [sea of mediterranea, mediterranian sea, roman...  Mediterranean Sea   \n",
       "3  [moonless, earth and moon, lunar mass, luna sa...           The moon   \n",
       "4  [thailand, kingdom of thailand, kingdom of tha...           Thailand   \n",
       "5                                     [between ribs]   Between the RIBS   \n",
       "6  [celebrity productions, iwerks ub, ub iwerks, ...          Ub Iwerks   \n",
       "7                                          [four, 4]               Four   \n",
       "8  [robocop 1987 film, robotic police officer, ro...            Robocop   \n",
       "9  [地震, earth quakes, earthquakes, tectonic earth...         Earthquake   \n",
       "\n",
       "                                    gold_aliases_raw  \n",
       "0  [Smoky City, Pittsburgh (Pa.), Pittsburgh, Pen...  \n",
       "1  [Australia (Commonwealth realm), AustraliA, Co...  \n",
       "2  [Mediterranian, Meditiranean, West Mediterrane...  \n",
       "3  [Sol 3a, Moon-like, Mass of the Moon, Solar an...  \n",
       "4  [Muang Thai, Taihland, ISO 3166-1:TH, Thai Emp...  \n",
       "5                                 [Between the RIBS]  \n",
       "6  [Iwerks, Ub, Ub Iwerks, Ub Iwerks Studio, Cele...  \n",
       "7                                    [Four, four, 4]  \n",
       "8  [I'd buy that for a dollar, RoboCop, RobotCop,...  \n",
       "9  [Seism, Earthquake, Seismic event, The kinds o...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_gold_normalized(ex):\n",
    "    \"\"\"\n",
    "    Возвращает список \"нормализованных\" корректных ответов.\n",
    "    В TriviaQA они уже подготовлены как normalized_aliases / normalized_value. [web:352][web:359]\n",
    "    \"\"\"\n",
    "    ans = ex[\"answer\"]\n",
    "    golds = ans.get(\"normalized_aliases\") or []\n",
    "    if not golds:\n",
    "        nv = ans.get(\"normalized_value\")\n",
    "        if nv:\n",
    "            golds = [nv]\n",
    "    return golds\n",
    "\n",
    "rows = []\n",
    "for ex in sample:\n",
    "    rows.append({\n",
    "        \"question_id\": ex.get(\"question_id\"),\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"gold_normalized\": extract_gold_normalized(ex),\n",
    "        \"gold_value_raw\": ex[\"answer\"].get(\"value\"),\n",
    "        \"gold_aliases_raw\": ex[\"answer\"].get(\"aliases\"),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ce21893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which US city was named after a British Prime Minister?\n",
      "Gold normalized: ['steel city', 'climate of pittsburgh pennsylvania', 'pittsbrugh', 'un locode uspit', 'pittsburgh', 'pittsburgh frick 6–8 middle school', 'pittsburgh pennsylvania usa', 'pittsburgh style', 'city of pittsburgh', 'pittsburgh pennsylvania u s', 'st justin s high school', 'pittsburgh pa', 'glenwood pennsylvania', 'da burgh', 'pittsburgh style of literature', 'pitsburgh', 'east end pittsburgh', 'pittsburgh pennsylvania us', 'pittsburgh usa', 'smoky city', 'city of bridges', 'fort du quesne', 'pittsburg pennsylvania', 'pittsburgh frick 6 8 middle school', 'pittsburgh pennsylvania', 'pittsburgh united states of america', 'education in pittsburgh', 'pittsburg pa', 'pittsburgh pennsyvania', 'burgh', 'frick international studies academy middle school', 'pittsburgh allegheny county pennsylvania', 'pittsburgh pgh']\n",
      "Gold raw value: Pittsburgh\n",
      "Gold raw aliases (first 5): ['Smoky City', 'Pittsburgh (Pa.)', 'Pittsburgh, Pennsylvania.', 'Frick International Studies Academy Middle School', 'Pitsburgh']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(\"Q:\", df.loc[i, \"question\"])\n",
    "print(\"Gold normalized:\", df.loc[i, \"gold_normalized\"])\n",
    "print(\"Gold raw value:\", df.loc[i, \"gold_value_raw\"])\n",
    "print(\"Gold raw aliases (first 5):\", (df.loc[i, \"gold_aliases_raw\"] or [])[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08484cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50c6b3efb194097a56e529ff43a0241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from: ../models/mistral-7b-instruct\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "DTYPE = torch.bfloat16 \n",
    "\n",
    "MODEL = \"../models/mistral-7b-instruct\" # \"mistralai/Mistral-7B-Instruct-v0.1\" # choose model name instead of path to local folder with model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=DTYPE,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"Loaded from:\", MODEL)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f6137a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about yourself. Who are you and how can you help me?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "attention_mask = torch.ones_like(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2314770a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: (1, 23)\n",
      "model.device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "input_ids = input_ids.to(model.device)  # Because device is auto\n",
    "attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "print(\"input_ids shape:\", tuple(input_ids.shape))\n",
    "print(\"model.device:\", model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fb1d69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Tell me about yourself. Who are you and how can you help me? [/INST] I am Mistral, a language model trained by the Mistral AI team. I am here to help you with any language-related task you might have. I can assist you with translation, writing, editing, and answering questions in a variety of languages. I can also help you with more specific tasks like finding information, making recommendations, and even generating creative content. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4be338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one(model, tokenizer, question, max_new_tokens=64, temperature=1.0, top_p=0.95):\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    out = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        return_dict_in_generate=True,\n",
    "        output_logits=True,   # важно: нужны logits на каждом шаге\n",
    "    )\n",
    "\n",
    "    # generated tokens только после prompt\n",
    "    prompt_len = input_ids.shape[1]\n",
    "    seq = out.sequences[0]\n",
    "    gen_tokens = seq[prompt_len:]  # (T,)\n",
    "    # logits: tuple length T, each (batch=1, vocab)\n",
    "    logits_steps = out.logits\n",
    "\n",
    "    # log p для каждого реально выбранного токена\n",
    "    token_logprobs = []\n",
    "    for t, step_logits in enumerate(logits_steps):\n",
    "        tok = gen_tokens[t].item()\n",
    "        lp = F.log_softmax(step_logits[0], dim=-1)[tok]\n",
    "        token_logprobs.append(lp)\n",
    "\n",
    "    # средний log-likelihood токенов (как у них в коде: np.mean(log_lik)) [web:596]\n",
    "    avg_token_loglik = torch.stack(token_logprobs).mean().item() if len(token_logprobs) else float(\"-inf\")\n",
    "\n",
    "    text = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "    return text, avg_token_loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5951c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_entropy_for_question(\n",
    "    question: str,\n",
    "    responses: list[str],\n",
    "    avg_token_logliks: list[float],\n",
    "    entailment_model,\n",
    "    strict_entailment: bool = False,\n",
    "):\n",
    "    example = {\"question\": question}\n",
    "\n",
    "    semantic_ids = get_semantic_ids(\n",
    "        responses,\n",
    "        model=entailment_model,\n",
    "        strict_entailment=strict_entailment,\n",
    "        example=example,\n",
    "    )\n",
    "\n",
    "    # p(cluster) по нормализованной сумме вероятностей ответов в кластере [web:596][web:594]\n",
    "    logp_per_cluster = logsumexp_by_id(semantic_ids, np.array(avg_token_logliks), agg=\"sum_normalized\")\n",
    "    sem_entropy = predictive_entropy_rao(np.array(logp_per_cluster))\n",
    "\n",
    "    return {\n",
    "        \"semantic_ids\": semantic_ids,\n",
    "        \"semantic_entropy\": float(sem_entropy),\n",
    "        \"logp_per_cluster\": logp_per_cluster,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0d20eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import semantic_uncertainty.uncertainty.uncertainty_measures.semantic_entropy as semantic_entropy\n",
    "\n",
    "# Перечитать модуль с диска\n",
    "importlib.reload(semantic_entropy)\n",
    "\n",
    "# Переимпортировать нужные объекты (чтобы они ссылались на обновлённый код)\n",
    "from semantic_uncertainty.uncertainty.uncertainty_measures.semantic_entropy import (\n",
    "    EntailmentDeberta,\n",
    "    EntailmentRoBERTa,\n",
    "    get_semantic_ids,\n",
    "    logsumexp_by_id,\n",
    "    predictive_entropy_rao,\n",
    ")\n",
    "\n",
    "# Проверка\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c8612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912be29dd4274539a1cd493a5b79f22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b1faf513cb4c9b8629cd35ba0f3545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e571775a554b6b99d11ae7426dcde0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df6f8d57e8c4412a05bac4b33b6e212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a5eff605034d898e594cefb8c389f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f076d911ae405ca7909a8136ff28b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ent_model = EntailmentRoBERTa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c17c56d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'endswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# entailment модель для кластеризации смыслов (локально) [web:594]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m ent_model = \u001b[43mEntailmentDeberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m M = \u001b[32m10\u001b[39m  \u001b[38;5;66;03m# число сэмплов ответов на один вопрос (для демо)\u001b[39;00m\n\u001b[32m      5\u001b[39m results = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/internal_probing/external/semantic_uncertainty/semantic_uncertainty/uncertainty/uncertainty_measures/semantic_entropy.py:28\u001b[39m, in \u001b[36mEntailmentDeberta.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmicrosoft/deberta-v2-xlarge-mnli\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmicrosoft/deberta-v2-xlarge-mnli\u001b[39m\u001b[33m\"\u001b[39m).to(DEVICE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/internal_probing/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1175\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1172\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/internal_probing/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2113\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2110\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2111\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/internal_probing/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2359\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2357\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2359\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2361\u001b[39m     logger.info(\n\u001b[32m   2362\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2363\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2364\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/internal_probing/.venv/lib/python3.12/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py:103\u001b[39m, in \u001b[36mDebertaV2TokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, do_lower_case, split_by_punct, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     90\u001b[39m     vocab_file=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    101\u001b[39m     **kwargs,\n\u001b[32m    102\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_lower_case\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_lower_case\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_by_punct\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_by_punct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28mself\u001b[39m.do_lower_case = do_lower_case\n\u001b[32m    119\u001b[39m     \u001b[38;5;28mself\u001b[39m.split_by_punct = split_by_punct\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/internal_probing/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mvocab_file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/internal_probing/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1857\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1855\u001b[39m     converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[32m   1856\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m converter_class(transformer_tokenizer).converted()\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mtekken.json\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1858\u001b[39m     transformer_tokenizer.original_tokenizer = transformer_tokenizer\n\u001b[32m   1859\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Mistral tekken.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'endswith'"
     ]
    }
   ],
   "source": [
    "# entailment модель для кластеризации смыслов (локально) [web:594]\n",
    "ent_model = EntailmentDeberta()\n",
    "\n",
    "M = 10  # число сэмплов ответов на один вопрос (для демо)\n",
    "results = []\n",
    "\n",
    "for ex in sample:  # sample = 10 items из шага 1\n",
    "    q = ex[\"question\"]\n",
    "\n",
    "    responses = []\n",
    "    avg_lls = []\n",
    "    for _ in range(M):\n",
    "        ans, avg_ll = generate_one(\n",
    "            model, tokenizer, q,\n",
    "            max_new_tokens=64,\n",
    "            temperature=1.0,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "        responses.append(ans)\n",
    "        avg_lls.append(avg_ll)\n",
    "\n",
    "    sem = semantic_entropy_for_question(\n",
    "        question=q,\n",
    "        responses=responses,\n",
    "        avg_token_logliks=avg_lls,\n",
    "        entailment_model=ent_model,\n",
    "        strict_entailment=False,\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"responses\": responses,\n",
    "        \"avg_token_logliks\": avg_lls,\n",
    "        \"semantic_ids\": sem[\"semantic_ids\"],\n",
    "        \"semantic_entropy\": sem[\"semantic_entropy\"],\n",
    "    })\n",
    "\n",
    "len(results), results[0][\"semantic_entropy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d709efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(\"Q:\", results[i][\"question\"])\n",
    "print(\"Semantic entropy:\", results[i][\"semantic_entropy\"])\n",
    "print()\n",
    "\n",
    "for r, sid in zip(results[i][\"responses\"], results[i][\"semantic_ids\"]):\n",
    "    print(f\"[cluster {sid}] {r}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
