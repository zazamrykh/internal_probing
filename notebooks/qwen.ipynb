{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ce8242",
   "metadata": {},
   "source": [
    "That is draft raw dirty code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "827726ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902d280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9578a04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked, \"Who are you?\" I need to respond in a way that's friendly and helpful. Let me start by acknowledging their question. I should mention my name and\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Who are you?'\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6315e502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th generation: <think>\n",
      "Okay, the user asked, \"Who are you?\" I need to respond appropriately. First, I should acknowledge their question. Then, I should explain that I'm an AI assistant designed to\n",
      "1-th generation: <think>\n",
      "Okay, so the user is asking if AI will make humanity slave. Hmm, that's a profound question. Let me start by breaking it down. First, I need to address the core\n"
     ]
    }
   ],
   "source": [
    "# multiple prompts case\n",
    "prompts = ['Who are you?', 'Will AI make humanity slave?']\n",
    "messages = [\n",
    "    [{\"role\": \"user\", \"content\": prompt}]\n",
    "    for prompt in prompts\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    padding_side='left'\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f'{i}-th generation: {tokenizer.decode(output[inputs[\"input_ids\"].shape[-1]:])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6af1e6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [15191, 525, 498, 30], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': tensor([[151643, 151643, 151644,    872,    198,  15191,    525,    498,     30,\n",
      "         151645,    198, 151644,  77091,    198],\n",
      "        [151644,    872,    198,   9945,  15235,   1281,  21941,  20362,     30,\n",
      "         151645,    198, 151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "<|endoftext|><|endoftext|><|im_start|>user\n",
      "Who are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(prompt))\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab9109e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'qwen3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('qwen config')\n",
    "model.config.model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f31ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95, 'pad_token_id': 151643, 'bos_token_id': 151643, 'eos_token_id': [151645, 151643]}. If this is not desired, please set these values explicitly.\n"
     ]
    }
   ],
   "source": [
    "# Try to parse generated scores\n",
    "gen_cfg = GenerationConfig(num_beams=1,\n",
    "                            do_sample=False,\n",
    "                            return_dict_in_generate=True,\n",
    "                            # output_scores=True,\n",
    "                            output_logits=True)\n",
    "\n",
    "prompts = ['Hello!', 'Tell me about Stepan Razin']\n",
    "\n",
    "messages = [\n",
    "    [{\"role\": \"user\", \"content\": prompt}]\n",
    "    for prompt in prompts\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    padding_side='left',\n",
    "    enable_thinking=False\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=10, generation_config=gen_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87da8c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_start|>user\n",
      "Hello!<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist you today?<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<|im_start|>user\n",
      "Tell me about Stepan Razin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Stepan Razin is a prominent figure in the Russian-speaking world, known for his role in the Russian Federation's political and cultural landscape. Here are some key points about him:\n",
      "\n",
      "1. **Political Career**: Stepan Razin was a significant figure in the Russian political system. He served as the Prime Minister of Russia from 2012 to 2022, which marked a notable period in the country's history. He was also the leader of the \"Black Sea\" movement, which aimed to push for a more diverse and economically developed Russia.\n",
      "\n",
      "2. **Cultural and Social Impact**: Razin is celebrated for his contributions to Russian culture, including his work in the arts and education. He has been involved in various social initiatives, such as supporting education and promoting cultural values.\n",
      "\n",
      "3. **Personal Life**: Razin is known for his personal life, which includes a family of four. He is married to his wife, and they have two children.\n",
      "\n",
      "4. **International Recognition**: He has been involved in international events and has received recognition for his work in the field of politics and culture.\n",
      "\n",
      "5. **Current Status**: As of 2023, Razin is still active in his political career, though he has since left office. His legacy continues to influence Russian politics and society.\n",
      "\n",
      "If you have any specific questions about his work or personal life, feel free to ask!<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "for seq in outputs['sequences']:\n",
    "    print(tokenizer.decode(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b583d2a",
   "metadata": {},
   "source": [
    "I want get list of only generated indexes and corresponding scores for each generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e29478ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_start|>user\n",
      "Hello!<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist you today?<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<|im_start|>user\n",
      "Tell me about Stepan Razin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Stepan Razin is a prominent figure in the Russian-speaking world, known for his role in the Russian Federation's political and cultural landscape. Here are some key points about him:\n",
      "\n",
      "1. **Political Career**: Stepan Razin was a significant figure in the Russian political system. He served as the Prime Minister of Russia from 2012 to 2022, which marked a notable period in the country's history. He was also the leader of the \"Black Sea\" movement, which aimed to push for a more diverse and economically developed Russia.\n",
      "\n",
      "2. **Cultural and Social Impact**: Razin is celebrated for his contributions to Russian culture, including his work in the arts and education. He has been involved in various social initiatives, such as supporting education and promoting cultural values.\n",
      "\n",
      "3. **Personal Life**: Razin is known for his personal life, which includes a family of four. He is married to his wife, and they have two children.\n",
      "\n",
      "4. **International Recognition**: He has been involved in international events and has received recognition for his work in the field of politics and culture.\n",
      "\n",
      "5. **Current Status**: As of 2023, Razin is still active in his political career, though he has since left office. His legacy continues to influence Russian politics and society.\n",
      "\n",
      "If you have any specific questions about his work or personal life, feel free to ask!<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "for seq in outputs['sequences']:\n",
    "    print(tokenizer.decode(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75bcf0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_output(outputs, input_ids, tokenizer):\n",
    "    trunc_seq_list = []\n",
    "\n",
    "    for i, seq in enumerate(outputs['sequences']):\n",
    "        first_idx = len(input_ids[i])\n",
    "        last_idx = torch.nonzero(seq == tokenizer.eos_token_id)[-1]\n",
    "        trunc_seq = seq[first_idx : last_idx + 1]\n",
    "\n",
    "        trunc_seq_list.append(trunc_seq)\n",
    "    scores_list = [[] for _ in range(len(trunc_seq_list))]\n",
    "\n",
    "    for step, scores in enumerate(outputs['scores']):\n",
    "        for i, seq in enumerate(trunc_seq_list):\n",
    "            if step >= len(seq):\n",
    "                continue\n",
    "\n",
    "            scores_list[i].append(scores[scores != float('-inf')][i].item())\n",
    "\n",
    "    scores_tensors_list = []\n",
    "    for trunc_seq, scores in zip(trunc_seq_list, scores_list):\n",
    "        assert len(trunc_seq_list) == len(scores_list)\n",
    "        scores_tensors_list.append(torch.tensor(scores))\n",
    "\n",
    "    return trunc_seq_list, scores_tensors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "187f1386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_output(outputs, input_ids, tokenizer, return_log_prob=True):\n",
    "    trunc_seq_list = []\n",
    "\n",
    "    for i, seq in enumerate(outputs['sequences']):\n",
    "        first_idx = len(input_ids[i])\n",
    "        if seq[-1] == tokenizer.eos_token_id or seq[-1] == tokenizer.pad_token_id:\n",
    "            last_idx = torch.nonzero(seq == tokenizer.eos_token_id)[-1]\n",
    "        else:\n",
    "            last_idx = len(seq)\n",
    "        trunc_seq = seq[first_idx : last_idx + 1]\n",
    "\n",
    "        trunc_seq_list.append(trunc_seq)\n",
    "\n",
    "    result_list = [[] for _ in range(len(trunc_seq_list))]\n",
    "    for step, logits in enumerate(outputs['logits']):\n",
    "        for i, seq in enumerate(trunc_seq_list):\n",
    "            if step >= len(seq):\n",
    "                continue\n",
    "\n",
    "            if return_log_prob:\n",
    "                result_list[i].append(F.log_softmax(logits[i], dim=0)[seq[step]])\n",
    "            else:\n",
    "                result_list[i].append(logits[i])\n",
    "\n",
    "    if not return_log_prob:\n",
    "        return trunc_seq_list, result_list\n",
    "\n",
    "    result_tensors_list = []\n",
    "    for trunc_seq, scores in zip(trunc_seq_list, result_list):\n",
    "        assert len(trunc_seq) == len(scores)\n",
    "        result_tensors_list.append(torch.tensor(scores))\n",
    "\n",
    "    return trunc_seq_list, result_tensors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18b5015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_seq_list, scores_list = truncate_output(outputs, inputs['input_ids'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d2cc7835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9989, 0.9987, 0.7887, 0.9995, 0.9995, 0.9483, 0.9999, 0.9721, 0.9997,\n",
       "        0.8988])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_list[0].exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b21ca2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_seq_list = []\n",
    "\n",
    "for seq in outputs['sequences']:\n",
    "    first_idx = len(inputs['input_ids'][0])\n",
    "    last_idx = torch.nonzero(seq == tokenizer.eos_token_id)[-1]\n",
    "    trunc_seq = seq[first_idx : last_idx + 1]\n",
    "\n",
    "    trunc_seq_list.append(trunc_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3b2b610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list = [[] for _ in range(len(trunc_seq_list))]\n",
    "\n",
    "for step, scores in enumerate(outputs['scores']):\n",
    "    for i, seq in enumerate(trunc_seq_list):\n",
    "        if step >= len(seq):\n",
    "            continue\n",
    "\n",
    "        scores_list[i].append(scores[scores != float('-inf')][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9b023914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(52.2759, device='cuda:0')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['scores'][0][outputs['scores'][0] != float('-inf')][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f09f5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([51.7064], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "one_step_scores = outputs['scores'][0][0]\n",
    "print(one_step_scores[one_step_scores != float('-inf')])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
